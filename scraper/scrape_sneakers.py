import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
import re

class SneakerScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.releases = []
    
    def scrape_nike_snkrs(self):
        """Scrape Nike SNKRS for upcoming releases"""
        print("üîç Scraping Nike SNKRS...")
        try:
            # Simulating Nike SNKRS data (actual scraping would require handling their API/dynamic content)
            nike_releases = [
                {
                    "brand": "Nike",
                    "name": "Air Jordan 1 Retro High OG 'Chicago Lost and Found'",
                    "price": "$170",
                    "release_date": "2024-01-15",
                    "image_url": "https://static.nike.com/a/images/t_PDP_1728_v1/f_auto,q_auto:eco/61734ec7-dad8-40f3-9b95-c7500939150a/air-jordan-1-retro-high-og-shoes-Pph9wD.png",
                    "website": "Nike SNKRS"
                },
                {
                    "brand": "Nike",
                    "name": "Nike Dunk Low 'Panda'",
                    "price": "$100",
                    "release_date": "2024-01-20",
                    "image_url": "https://static.nike.com/a/images/t_PDP_1728_v1/f_auto,q_auto:eco/dd1391-100/dunk-low-shoes-5FQWGR.png",
                    "website": "Nike SNKRS"
                }
            ]
            self.releases.extend(nike_releases)
            print(f"‚úÖ Found {len(nike_releases)} Nike releases")
        except Exception as e:
            print(f"‚ùå Error scraping Nike: {e}")
    
    def scrape_adidas(self):
        """Scrape Adidas for upcoming releases"""
        print("üîç Scraping Adidas...")
        try:
            # Simulating Adidas data
            adidas_releases = [
                {
                    "brand": "Adidas",
                    "name": "Yeezy Boost 350 V2 'Bone'",
                    "price": "$230",
                    "release_date": "2024-01-18",
                    "image_url": "https://assets.adidas.com/images/h_840,f_auto,q_auto,fl_lossy,c_fill,g_auto/8a2f41f6f9d04d6a8c5aaf8d00e8c7e2_9366/Yeezy_Boost_350_V2_Shoes_White_HQ4540_01_standard.jpg",
                    "website": "Adidas"
                },
                {
                    "brand": "Adidas",
                    "name": "Forum Low 'Cloud White'",
                    "price": "$90",
                    "release_date": "2024-01-22",
                    "image_url": "https://assets.adidas.com/images/h_840,f_auto,q_auto,fl_lossy,c_fill,g_auto/28b0f1c6c8a64d6f9c5aaf8d00e8c7e2_9366/Forum_Low_Shoes_White_FY7757_01_standard.jpg",
                    "website": "Adidas"
                }
            ]
            self.releases.extend(adidas_releases)
            print(f"‚úÖ Found {len(adidas_releases)} Adidas releases")
        except Exception as e:
            print(f"‚ùå Error scraping Adidas: {e}")
    
    def scrape_footlocker(self):
        """Scrape Foot Locker for upcoming releases"""
        print("üîç Scraping Foot Locker...")
        try:
            # Simulating Foot Locker data
            footlocker_releases = [
                {
                    "brand": "Jordan",
                    "name": "Air Jordan 4 Retro 'Black Cat'",
                    "price": "$210",
                    "release_date": "2024-01-25",
                    "image_url": "https://images.footlocker.com/is/image/FLEU/314254001_01?wid=581&hei=581&fmt=png-alpha",
                    "website": "Foot Locker"
                },
                {
                    "brand": "Nike",
                    "name": "Air Max 90 'Infrared'",
                    "price": "$130",
                    "release_date": "2024-01-28",
                    "image_url": "https://images.footlocker.com/is/image/FLEU/325213126_01?wid=581&hei=581&fmt=png-alpha",
                    "website": "Foot Locker"
                }
            ]
            self.releases.extend(footlocker_releases)
            print(f"‚úÖ Found {len(footlocker_releases)} Foot Locker releases")
        except Exception as e:
            print(f"‚ùå Error scraping Foot Locker: {e}")
    
    def scrape_stockx(self):
        """Scrape StockX for trending releases"""
        print("üîç Scraping StockX...")
        try:
            # Simulating StockX data
            stockx_releases = [
                {
                    "brand": "Travis Scott",
                    "name": "Travis Scott x Air Jordan 1 Low OG 'Olive'",
                    "price": "$1,500",
                    "release_date": "2024-02-01",
                    "image_url": "https://images.stockx.com/images/Air-Jordan-1-Low-OG-SP-Travis-Scott-Olive.jpg?fit=fill&bg=FFFFFF&w=700&h=500&fm=webp&auto=compress&q=90&dpr=2&trim=color&updated_at=1638316684",
                    "website": "StockX"
                },
                {
                    "brand": "Off-White",
                    "name": "Off-White x Nike Dunk Low 'Pine Green'",
                    "price": "$2,200",
                    "release_date": "2024-02-05",
                    "image_url": "https://images.stockx.com/images/Nike-Dunk-Low-Off-White-Pine-Green.jpg?fit=fill&bg=FFFFFF&w=700&h=500&fm=webp&auto=compress&q=90&dpr=2&trim=color&updated_at=1638316684",
                    "website": "StockX"
                }
            ]
            self.releases.extend(stockx_releases)
            print(f"‚úÖ Found {len(stockx_releases)} StockX releases")
        except Exception as e:
            print(f"‚ùå Error scraping StockX: {e}")
    
    def scrape_goat(self):
        """Scrape GOAT for upcoming releases"""
        print("üîç Scraping GOAT...")
        try:
            # Simulating GOAT data
            goat_releases = [
                {
                    "brand": "New Balance",
                    "name": "New Balance 550 'White Green'",
                    "price": "$110",
                    "release_date": "2024-02-08",
                    "image_url": "https://image.goat.com/attachments/product_template_pictures/images/008/654/900/original/550_White_Green.png",
                    "website": "GOAT"
                },
                {
                    "brand": "Converse",
                    "name": "Converse Chuck 70 High 'Comme des Gar√ßons'",
                    "price": "$150",
                    "release_date": "2024-02-10",
                    "image_url": "https://image.goat.com/attachments/product_template_pictures/images/008/654/901/original/Chuck_70_CDG.png",
                    "website": "GOAT"
                }
            ]
            self.releases.extend(goat_releases)
            print(f"‚úÖ Found {len(goat_releases)} GOAT releases")
        except Exception as e:
            print(f"‚ùå Error scraping GOAT: {e}")
    
    def format_release_date(self, date_str):
        """Format release date for better readability"""
        try:
            date_obj = datetime.strptime(date_str, "%Y-%m-%d")
            return date_obj.strftime("%B %d, %Y")
        except:
            return date_str
    
    def sort_releases_by_date(self):
        """Sort releases by release date"""
        try:
            self.releases.sort(key=lambda x: datetime.strptime(x['release_date'], "%Y-%m-%d"))
        except:
            pass
    
    def display_releases(self):
        """Display all scraped releases in a formatted way"""
        print("\n" + "="*80)
        print("üî• UPCOMING SNEAKER RELEASES üî•")
        print("="*80)
        
        if not self.releases:
            print("No releases found.")
            return
        
        self.sort_releases_by_date()
        
        for i, release in enumerate(self.releases, 1):
            print(f"\n{i}. {release['name']}")
            print(f"   Brand: {release['brand']}")
            print(f"   Price: {release['price']}")
            print(f"   Release Date: {self.format_release_date(release['release_date'])}")
            print(f"   Website: {release['website']}")
            print(f"   Image: {release['image_url'][:60]}...")
            print("-" * 60)
    
    def save_to_json(self, filename="sneaker_releases.json"):
        """Save releases to JSON file"""
        try:
            with open(filename, 'w') as f:
                json.dump(self.releases, f, indent=2)
            print(f"\nüíæ Data saved to {filename}")
        except Exception as e:
            print(f"‚ùå Error saving to JSON: {e}")
    
    def get_release_stats(self):
        """Display statistics about the scraped releases"""
        if not self.releases:
            return
        
        print(f"\nüìä RELEASE STATISTICS")
        print(f"Total releases found: {len(self.releases)}")
        
        # Count by brand
        brands = {}
        websites = {}
        
        for release in self.releases:
            brand = release['brand']
            website = release['website']
            
            brands[brand] = brands.get(brand, 0) + 1
            websites[website] = websites.get(website, 0) + 1
        
        print(f"\nReleases by brand:")
        for brand, count in sorted(brands.items()):
            print(f"  {brand}: {count}")
        
        print(f"\nReleases by website:")
        for website, count in sorted(websites.items()):
            print(f"  {website}: {count}")
    
    def run_scraper(self):
        """Run the complete scraping process"""
        print("üöÄ Starting sneaker release scraper...")
        print("Note: This is a demonstration with simulated data.")
        print("Real scraping would require handling dynamic content, APIs, and anti-bot measures.\n")
        
        # Scrape all websites
        self.scrape_nike_snkrs()
        time.sleep(1)  # Be respectful with requests
        
        self.scrape_adidas()
        time.sleep(1)
        
        self.scrape_footlocker()
        time.sleep(1)
        
        self.scrape_stockx()
        time.sleep(1)
        
        self.scrape_goat()
        
        # Display results
        self.display_releases()
        self.get_release_stats()
        self.save_to_json()

# Run the scraper
if __name__ == "__main__":
    scraper = SneakerScraper()
    scraper.run_scraper()
